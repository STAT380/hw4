---
title: "Homework 4"
author: "[Diana Batista Capellan]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: html
#format: pdf
editor: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

::: {.callout-important style="font-size: 0.8em;"}
Please read the instructions carefully before submitting your
assignment.

1.  This assignment requires you to only upload a `PDF` file on Canvas
2.  Don't collapse any code cells before submitting.
3.  Remember to make sure all your code output is rendered properly
    before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter
before submitting your assignment ⚠️
:::

We will be using the following libraries:

```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom"
)

# renv::install(packages)
sapply(packages, require, character.only=T)
```

## <br><br><br><br>

## Question 1

::: callout-tip
## 30 points

Automatic differentiation using `torch`
:::

###### 1.1 (5 points)

Consider $g(x, y)$ given by $$
g(x, y) = (x - 3)^2 + (y - 4)^2.
$$

Using elementary calculus derive the expressions for

$$
\frac{d}{dx}g(x, y) = 2(x-3) , \quad \text{and} \quad \frac{d}{dy}g(x, y) = 2(y-4).
$$

Using your answer from above, what is the answer to $$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} = 2(3-3)=0 \quad \text{and} \quad \frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)} = 2(4-4)= 0
$$

Define $g(x, y)$ as a function in R, compute the gradient of $g(x, y)$
with respect to $x=3$ and $y=4$. Does the answer match what you
expected?

```{r}
#install.packages("numDeriv")
library(numDeriv)

# Define the function g(x, y)
g <- function(z) {
  x <- z[1]
  y <- z[2]
  return((x - 3)^2 + (y - 4)^2)
}

# Compute the gradient of g(x, y) at (x=3, y=4)
gradient <- grad(g, c(3, 4))

# Extract partial derivatives
gradient_x <- gradient[1]  # Partial derivative with respect to x
gradient_y <- gradient[2]  # Partial derivative with respect to y

print(gradient_x)
print(gradient_y)

```

**My calculations match the output**

###### 1.2 (10 points)

$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$

Consider $h(\u, \v)$ given by $$
h(\u, \v) = (\u \cdot \v)^3,
$$ where $\u \cdot \v$ denotes the dot product of two vectors, i.e.,
$\u \cdot \v = \sum_{i=1}^n u_i v_i.$

Using elementary calculus derive the expressions for the gradients

$$
\begin{aligned}
\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), \dots, \frac{d}{du_n}h(\u, \v)\Bigg) = 3(u⋅v)2v_i​
\end{aligned}
$$

**Answer in the expression**

Using your answer from above, what is the answer to
$\nabla_\u h(\u, \v)$ when $n=10$ and

$$
\begin{aligned}
\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
\end{aligned}
$$

$$
\begin{aligned}
\ 3(u⋅v)2v_i = 3(2)2v_1 = ?
\end{aligned}
$$

Define $h(\u, \v)$ as a function in R, initialize the two vectors $\u$
and $\v$ as `torch_tensor`s. Compute the gradient of $h(\u, \v)$ with
respect to $\u$. Does the answer match what you expected?

```{r}
# Define the function h(u, v)
h <- function(u, v) {
  return((torch_dot(u, v))^3)
}

# Initialize the vectors u and v as torch_tensors
u <- torch_tensor(c(-1, +1, -1, +1, -1, +1, -1, +1, -1, +1), dtype = torch_float())
v <- torch_tensor(c(-1, -1, -1, -1, -1, +1, +1, +1, +1, +1), dtype = torch_float())

# Define a function to compute the gradient of h(u, v) with respect to u
gradient_h_u <- function(u, v) {
  h_value <- h(u, v)
  gradient <- grad(h_value, u)
  return(gradient)
}

```

------------------------------------------------------------------------

###### 1.3 (5 points)

Consider the following function $$
f(z) = z^4 - 6z^2 - 3z + 4
$$

Derive the expression for $$
f'(z_0) = \frac{df}{dz}\Bigg|_{z=z_0}
$$ and evaluate $f'(z_0)$ when $z_0 = -3.5$.

Define $f(z)$ as a function in R, and using the `torch` library compute
$f'(-3.5)$.

------------------------------------------------------------------------

###### 1.4 (5 points)

For the same function $f$, initialize $z[1] = -3.5$, and perform $n=100$
iterations of **gradient descent**, i.e.,

> \$z\[{k+1}\] = z\[k\] - \eta f'(z\[k\])    \$ for
> $k = 1, 2, \dots, 100$

Plot the curve $f$ and add taking $\eta = 0.02$, add the points
$\{z_0, z_1, z_2, \dots z_{100}\}$ obtained using gradient descent to
the plot. What do you observe?

------------------------------------------------------------------------

###### 1.5 (5 points)

Redo the same analysis as **Question 1.4**, but this time using
$\eta = 0.03$. What do you observe? What can you conclude from this
analysis

<br><br><br><br> <br><br><br><br> ---

## Question 2

::: callout-tip
## 50 points

Logistic regression and interpretation of effect sizes
:::

For this question we will use the **Titanic** dataset from the Stanford
data archive. This dataset contains information about passengers aboard
the Titanic and whether or not they survived.

------------------------------------------------------------------------

###### 2.1 (5 points)

Read the data from the following URL as a tibble in R. Preprocess the
data such that the variables are of the right data type, e.g., binary
variables are encoded as factors, and convert all column names to lower
case for consistency. Let's also rename the response variable `Survival`
to `y` for convenience.

```{r}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read_csv(url) %>%
  mutate_if(is.logical, as.factor) %>%  # Convert logical columns to factors
  mutate(across(everything(), as.character)) %>%  # Convert all columns to character type
  mutate(across(c("Survived", "Pclass", "Age", 
                  "Siblings/Spouses Aboard", "Parents/Children Aboard", 
                  "Fare"), as.factor)) %>%  # Convert specified columns to factors
  rename(y = Survived) %>%  # Rename response variable to 'y'
  rename_all(tolower)  # Convert column names to lower case

head(df)
```

------------------------------------------------------------------------

###### 2.2 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using
`corrplot()`

```{r}
df %>% 
  select(all_of(c("y", "pclass", "age", 
                  "siblings/spouses aboard", 
                  "parents/children aboard", "fare"))) %>%  # Select columns by name
  mutate(across(c("y", "pclass", "age", 
                  "siblings/spouses aboard", 
                  "parents/children aboard", "fare"), as.numeric)) %>%  # Convert selected numeric columns to numeric
  cor() %>%  # Calculate the correlation matrix
  corrplot(method = "color")  # Visualize the correlation matrix
```

------------------------------------------------------------------------

###### 2.3 (10 points)

Fit a logistic regression model to predict the probability of surviving
the titanic as a function of:

-   `pclass`
-   `sex`
-   `age`
-   `fare`
-   `# siblings`
-   `# parents`

```{r}

full_model <- glm(y ~ pclass + sex + age + fare + `siblings/spouses aboard` + `parents/children aboard`, data = df, family = binomial) # Insert your code here

summary(full_model)
```

------------------------------------------------------------------------

###### 2.4 (30 points)

Provide an interpretation for the slope and intercept terms estimated in
`full_model` in terms of the log-odds of survival in the titanic and in
terms of the odds-ratio (if the covariate is also categorical).

::: callout-hint
## 

Recall the definition of logistic regression from the lecture notes, and
also recall how we interpreted the slope in the linear regression model
(particularly when the covariate was categorical).
:::

```{r}
print("The coefficients (slopes) represent the change 
in log-odds of the response variable for a one-unit change in the predictor variable.

For categorical predictors, coefficients indicate the change 
in log-odds compared to the reference category.

The intercept represents the log-odds of survival when 
all predictors are zero.

Coefficients can be interpreted in terms of odds ratios for 
categorical predictors, showing the multiplicative change in odds 
compared to the reference category.")
```

<br><br><br><br> <br><br><br><br> ---

## Question 3

::: callout-tip
## 70 points

Variable selection and logistic regression in `torch`
:::

------------------------------------------------------------------------

###### 3.1 (15 points)

Complete the following function `overview` which takes in two
categorical vectors (`predicted` and `expected`) and outputs:

-   The prediction accuracy
-   The prediction error
-   The false positive rate, and
-   The false negative rate

```{r}
overview <- function(predicted, expected){
    accuracy <- sum(predicted == expected) / length(expected) # Insert your code here
    error <- 1 - accuracy # Insert your code here
    
    confusion_matrix <- table(expected, predicted)
    
    total_false_positives <- confusion_matrix["0", "1"] # Insert your code here
    total_true_positives <- confusion_matrix["1", "1"] # Insert your code here
    total_false_negatives <- confusion_matrix["1", "0"] # Insert your code here
    total_true_negatives <- confusion_matrix["0", "0"] # Insert your code here
    false_positive_rate <- total_false_positives / (total_false_positives + total_true_negatives) # Insert your code here
    false_negative_rate <- total_false_negatives / (total_false_negatives + total_true_positives) # Insert your code here
    return(
        data.frame(
            accuracy = accuracy, 
            error=error, 
            false_positive_rate = false_positive_rate, 
            false_negative_rate = false_negative_rate
        )
    )
}
```

You can check if your function is doing what it's supposed to do by
evaluating

```{r}
overview(df$y, df$y)
```

## and making sure that the accuracy is $100\%$ while the errors are $0\%$.

###### 3.2 (5 points)

Display an overview of the key performance metrics of `full_model`

```{r}
summary(full_model) # Insert your code here
```

------------------------------------------------------------------------

###### 3.3 (5 points)

Using backward-stepwise logistic regression, find a parsimonious
altenative to `full_model`, and print its `overview`

```{r}
step_model <- step(full_model, direction = "backward") # Insert your code here. 
summary(step_model)
```

```{r}
step_predictions <- predict(step_model, type = "response")

# Convert probabilities to binary predictions
step_predictions <- ifelse(step_predictions >= 0.5, 1, 0)

# Overview of the performance metrics
overview(step_predictions, df$y)
```

------------------------------------------------------------------------

###### 3.4 (15 points)

Using the `caret` package, setup a $5$-fold cross-validation training
method using the `caret::trainConrol()` function

```{r}
controls <- trainControl(method = "cv", number = 5) # insert your code here
```

Now, using `control`, perform $5$-fold cross validation using
`caret::train()` to select the optimal $\lambda$ parameter for. LASSO
with logistic regression.

Take the search grid for $\lambda$ to be in
$\{ 2^{-20}, 2^{-19.5}, 2^{-19}, \dots, 2^{-0.5}, 2^{0} \}$.

```{r}
# Insert your code in the ... region
lasso_fit <- train(
  x = df[, c("pclass", "sex", "age", "siblings/spouses aboard", "parents/children aboard", "fare")],
  y = df$y,
  method = "glmnet",
  trControl = controls, 
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = 2^seq(-20, 0, by = 0.5)
    ),
  family = "binomial"
)
```

Using the information stored in `lasso_fit$results`, plot the results
for cross-validation accuracy vs. $log_2(\lambda)$. Choose the optimal
$\lambda^*$, and report your results for this value of $\lambda^*$.

```{r}
# Plot cross-validation accuracy vs. lambda
plot(lasso_fit$results$lambda, lasso_fit$results$Accuracy, type = "l",
     xlab = "Lambda", ylab = "Cross-validation Accuracy", 
     main = "Cross-validation Accuracy vs. Lambda")

# Identify the optimal lambda
optimal_lambda <- lasso_fit$results$lambda[which.max(lasso_fit$results$Accuracy)]
points(optimal_lambda, max(lasso_fit$results$Accuracy), col = "red", pch = 20)
text(optimal_lambda, max(lasso_fit$results$Accuracy), 
     sprintf("Optimal Lambda: %f", optimal_lambda), pos = 3)

# Report results for the optimal lambda
best_accuracy <- max(lasso_fit$results$Accuracy)
cat("Optimal Lambda:", optimal_lambda, "\n")
cat("Cross-validation Accuracy with Optimal Lambda:", best_accuracy, "\n")

```

------------------------------------------------------------------------

###### 3.5 (25 points)

First, use the `model.matrix()` function to convert the covariates of
`df` to a matrix format

```{r}
covariate_matrix <- model.matrix(full_model)[, -1]
```

Now, initialize the covariates $X$ and the response $y$ as `torch`
tensors

```{r}
X <- torch_tensor(covariate_matrix, dtype = torch_float()) # Insert your code here
y <- torch_tensor(df$y, dtype = torch_float()) # Insert your code here

```

Using the `torch` library, initialize an `nn_module` which performs
logistic regression for this dataset. (Remember that we have 6 different
covariates)

```{r}
logistic <- nn_module(
  initialize = function() {
    self$f <- nn_linear(6, 1) # Insert your code here
    self$g <- nn_sigmoid() # Insert your code here
  },
  forward = function(x) {
   x <- x %>% 
      self$f() %>%             
      self$g() # Insert your code here
   return(x)
  }
)

f <- logistic()
```

You can verify that your code is right by checking that the output to
the following code is a vector of probabilities:

```{r}
#f(X)
```

Now, define the loss function `Loss()` which takes in two tensors `X`
and `y` and a function `Fun`, and outputs the **Binary cross Entropy
loss** between `Fun(X)` and `y`.

```{r}
Loss <- function(X, y, Fun){
  pred_probs <- Fun(X)
  
  # Compute binary cross-entropy loss
  loss <- -torch_mean(y * torch_log(pred_probs) + (1 - y) * torch_log(1 - pred_probs))
  
  return(loss) # Insert our code here
}
```

Initialize an optimizer using `optim_adam()` and perform $n=1000$ steps
of gradient descent in order to fit logistic regression using `torch`.

``` r
f <- logistic()
optimizer <- optim_adam(params = f$parameters(), lr = 0.01) # Insert your code here

n <- 1000
for (i in 1:n) {
  # Forward pass: compute predicted probabilities
  pred_probs <- f$forward(X)
  
  # Compute the binary cross-entropy loss
  loss <- Loss(X, y, pred_probs)
  
  # Backward pass: compute gradients
  optimizer$zero_grad()
  loss$backward()
  
  # Update parameters
  optimizer$step()
  
  # Print the loss every 100 steps
  if (i %% 100 == 0) {
    cat(sprintf("Step %d, Loss: %.4f\n", i, as_scalar(loss)))
  }} # Insert your code for gradient descent here
```

Using the final, optimized parameters of `f`, compute the compute the
predicted results on `X`

```{r}
predicted_probabilities <- c(1, 0, 0.5) #f(X) %>% as_array()
torch_predictions <- torch_tensor(predicted_probabilities, dtype = torch_float()) # Insert your code here

#overview(torch_predictions, df$y)
```

------------------------------------------------------------------------

###### 3.6 (5 points)

Create a summary table of the `overview()` summary statistics for each
of the $4$ models we have looked at in this assignment, and comment on
their relative strengths and drawbacks.

::: {.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br> <br><br><br><br> ---

::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
